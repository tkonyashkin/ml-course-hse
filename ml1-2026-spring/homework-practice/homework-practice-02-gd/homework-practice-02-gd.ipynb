{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3b01f93",
      "metadata": {
        "id": "a3b01f93"
      },
      "source": [
        "# Машинное обучение, ФКН ВШЭ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af12f230",
      "metadata": {
        "id": "af12f230"
      },
      "source": [
        "## Практическое домашнее задание 2. Градиентный спуск своими руками\n",
        "\n",
        "### Общая информация\n",
        "\n",
        "Дата выдачи: 09.02.2026\n",
        "\n",
        "Мягкий дедлайн: 23.02.2026 23:59 MSK\n",
        "\n",
        "Жесткий дедлайн: 01.03.2026 23:59 MSK\n",
        "\n",
        "\n",
        "### Оценивание и штрафы\n",
        "\n",
        "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). **Максимально допустимая оценка за работу — 10 баллов + 0.5 за социальный бонус.**\n",
        "\n",
        "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
        "\n",
        "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо считываемые диаграммы.\n",
        "\n",
        "Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
        "\n",
        "**Устная проверка.** Для проверки понимания кода и выводов студент может быть приглашён на устную защиту. Оценка за задание может быть изменена после устной защиты. Если студент не может объяснить ключевые части решения и принятые решения, работа считается недобросовестной и оценивается в 0 баллов независимо от автотестов.\n",
        "\n",
        "### Формат сдачи\n",
        "\n",
        "Задания сдаются через систему Anytask. Инвайт можно найти на странице курса. Присылать необходимо ноутбук с выполненным заданием, а также файлы `descents.py` и `linear_regression.py`. Сам ноутбук называйте в формате **homework-practice-02-gd-Username.ipynb**, где Username - ваша фамилия.\n",
        "\n",
        "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
        "\n",
        "**Оценка**: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3615ad5",
      "metadata": {
        "id": "a3615ad5"
      },
      "source": [
        "\n",
        "### О задании\n",
        "\n",
        "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных модификаций градиентного спуска. В файле `descents.py` вам нужно будет реализовать несколько классов для различных вариаций градиентного спуска, а именно:\n",
        "* `VanillaGradientDescent`\n",
        "* `StochasticGradientDescent`\n",
        "* `SAGDescent`\n",
        "* `MomentumDescent`\n",
        "* `Adam`\n",
        "\n",
        "\n",
        "В файле `linear_regression.py` вам необходимо будет реализовать класс `CustomLinearRegression` для обучения линейной регрессии (и, разумеется, предсказания целевой переменной на основе обученной модели).\n",
        "\n",
        "\n",
        "### Про предложенную архитектуру\n",
        "\n",
        "Предложенная вам архитектура шаблонов написана по принципам SOLID: основная ее идея в том, что вы сможете использовать различные лоссы и оптимизаторы с одним и тем же кодом прочих классов, никак не изменяя и не переписывая методы классов, которые с оптимизаторами и лоссом взаимодействуют. Мы добиваемся этого при помощи выделения интерфейсов (в Python мы достигаем этого при помощи абстрактных классов, см дальше в заданиях) и выделения зон ответственности каждого класса.\n",
        "\n",
        "Глобально в нашей архитектуре всего 4 интерфейса (некоторые из которых на самом деле сразу concrete классы), каждый из которых порождает одно семейство:\n",
        "- `interfaces.LossFunction`\n",
        "  - Классы, имплементирующие этот интерфейс, отвечают за одну конкретную функцию потерь, используемую при обучении, и всё, что меняется вместе с ней при её замене: подсчёт лосса, подсчет градиента и аналитическое решение (если есть, то добавляется в интерфейс соответствующим mixin-ом).\n",
        "- `interfaces.LinearRegressionInterface`\n",
        "  - Интерфейс обертки для модели линейной регрессии, контейнер, содержащий составные части (лосс-функцию и оптимизатор) и использующий их для выполнения содержательной работы.\n",
        "- `interfaces.LearningRateSchedule`\n",
        " - Простенькое семейство расписаний, определяющих шаг обучения для каждой итерации\n",
        "- `interfaces.AbstractOptimizer`.\n",
        "  -  Классы, имплементирующие этот интерфейс, имплементируют конкретный алгоритм оптимизации и всё, что происходит в его процессе. Пользуются обёрткой линейной регрессии для доступа к данным и вызова расчетов, чтобы не зависеть напрямую от конкретных функций потерь и шедулеров шага обучения.\n",
        "\n",
        "Концепция передачи маленьких объектов, отвечающих за свою маленькую зону ответственности внутрь более сложного объекта для выполнения ими составных частей работы называется Dependency Injection, и работает как раз за счет выделения зоны ответственности и опоры на интерфейс вместо реализации.\n",
        "\n",
        "Посмотрите на код `linear_regression.CustomLinearRegression`: она принимает в себя объекты с интерфейсами `LossFunction` и `AbstractOptimizer`, а в `descents.BaseDescent` как уточнении интерфейса абстрактного оптимизатора до итеративных оптимизаторов видно, что он в свою очередь принимает при инициализации объект шедулера шага обучения.\n",
        "\n",
        "Благодаря этому, код, который использует эти классы, может по очереди:\n",
        "- Инициализировать нужный шедулер с нужными параметрами для задания шага обучения\n",
        "- Иницализировать оптимизатор, задав ему нужные параметры процесса и передав готовый шедулер\n",
        "- Инициализировать класс линейной регрессии нужной под задачу функцией потерь и уже готовым оптимизатором.\n",
        "- Запустить процесс обучения\n",
        "\n",
        "И на любом этапе можно использовать другой объект с подходящим интерфейсом, и всё будет работать!\n",
        "\n",
        "\n",
        "**В ходе выполнения этой домашки вы наполните все эти семейства классов различными имплементациями и будете менять их на ходу как перчатки!**\n",
        "\n",
        "Более подробно про наследование классов в Python можно прочитать здесь:\n",
        "* Наследование: https://docs.python.org/3/tutorial/classes.html#inheritance\n",
        "* Абстрактные классы: https://docs.python.org/3/library/abc.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e10fc6",
      "metadata": {
        "id": "90e10fc6"
      },
      "source": [
        "## Задание 1. Линейная регрессия  (1 балл)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc07852",
      "metadata": {
        "id": "9cc07852"
      },
      "source": [
        "### Градиент функции потерь MSE\n",
        "\n",
        "На семинаре про [матрично-векторное дифференцирование](https://github.com/esokolov/ml-course-hse/blob/master/2025-fall/seminars/sem03-vector-diff.pdf) вы должны были обсуждать дифференцирование функции потерь MSE в матричном виде.\n",
        "\n",
        "### Задание 1.0. Градиент MSE в матричном виде (0.3 балла).\n",
        "\n",
        "Напомним, что функция потерь MSE записывается как:\n",
        "\n",
        "$$\n",
        "    Q(w) = \\frac{1}{\\ell} \\sum \\limits_{i = 1}^\\ell (y_i - \\langle x_i, w \\rangle)^2 = \\frac{1}{\\ell} \\| X w - y \\|^2\n",
        "$$\n",
        "\n",
        "где $\\ell$ – количество объектов в выборке, $X \\in \\mathbb{R}^{\\ell \\times d}$ – матрица \"объект-признак\", а $y \\in \\mathbb{R}^\\ell$ – целевая переменная. Через $x_i$ обозначается $i$-ая строчка матрицы $X$, отвечающая за $i$-й объект выборки.\n",
        "\n",
        "- **Выпишите ниже (подсмотрев в семинар или решив самостоятельно) градиент для функции потерь MSE в матричном виде.**\n",
        "\n",
        "**Решение:**\n",
        "\n",
        "`### your solution here (ಠ.ಠ) ###`\n",
        "\n",
        "\n",
        "- **Имплементируйте методы `MSELoss.loss`, `MSELoss.gradient`**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "024b0624",
      "metadata": {
        "id": "024b0624"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### Задание 1.1 Аналитическое решение и `CustomLinearRegression` (0.7 балла)\n",
        "\n",
        "Перед тем, как мы углубимся в итеративные методы оптимизации, давайте вспомним, что для ряда функций потерь существует и аналитическое решение. Давайте сперва вспомним, как оно выглядит для MSE.\n",
        "\n",
        "- **Выведите формулу оптимальных $w$ в задаче минимизации MSE, и запишите её ниже.**\n",
        "\n",
        "- **Имплементируйте подсчет этого решения в `MSELoss._plain_analytic_solution`**\n",
        "\n",
        "$$\\text{MSE} = \\| X w - y \\|^2$$\n",
        "$$ w = $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1272621b",
      "metadata": {
        "id": "1272621b"
      },
      "source": [
        "**Вопрос**: Как мы помним, у аналитического решения есть минусы - какие?\n",
        "\n",
        "**Ответ**: `# your text here (ಠ.ಠ)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037006e4",
      "metadata": {
        "id": "037006e4"
      },
      "source": [
        "Теперь прокинем это решение в наш класс линейной регрессии, чтобы получше разобраться в архитектуре.\n",
        "\n",
        "\n",
        "- **Допишите класс `descents.AnalyticSolutionOptimizer`**\n",
        "- **Допишите класс `CustomLinearRegression`**\n",
        "  - В нем на текущем этапе нужно имплементировать все методы: `fit` и `predict` вам понадобятся прямо сейчас, а `compute_gradients` и `compute_loss` в следующей части.\n",
        "\n",
        "Помните, про разделение ответственности классов!\n",
        "\n",
        "За контроль процесса обучения отвечает оптимизатор, а объект линейной регрессии по факту выступает точкой входа, контейнером для данных и способом доступа к вычислениям, на основе которых оптимизатор принимает решения (e.g. значение антиградиента в точке весов).\n",
        "\n",
        "При этом сам по себе оптимизатор должен быть универсален, в нем никак не должны содержаться детали, связанные с конкретными функциями потерь, все необходимое от них он может получить через `self.model`.\n",
        "\n",
        "Аналогично, класс линейной регрессии тоже должен быть универсальным и готовым к работе с любыми лоссами и оптимизаторами, исполняющими заявленный интерфейс. Здесь применена dependency injection, и вы должны грммотно ее поддержать в своей имплементации. Аналогично, все, что вам может быть нужно от функций потерь, вы можете получить при помощи обращения к переданному объекту."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4b1178",
      "metadata": {
        "id": "0e4b1178"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from linear_regression import MSELoss, CustomLinearRegression, AnalyticSolutionOptimizer\n",
        "\n",
        "num_objects = 100\n",
        "dimension = 5\n",
        "\n",
        "x = np.random.rand(num_objects, dimension)\n",
        "y = np.random.rand(num_objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb116268",
      "metadata": {
        "id": "cb116268"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "import sklearn\n",
        "\n",
        "sklearn_linreg = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
        "sklearn_linreg.fit(x, y)\n",
        "print(\"Sklearn MSE\", mse(sklearn_linreg.predict(x), y))\n",
        "\n",
        "your_linreg = CustomLinearRegression(AnalyticSolutionOptimizer(), loss_function=MSELoss())\n",
        "your_linreg.fit(x, y)\n",
        "print(\"Your MSE\", mse(your_linreg.predict(x), y))\n",
        "\n",
        "assert abs(mse(your_linreg.predict(x), y) - mse(sklearn_linreg.predict(x), y)) < 1e-12, \"Не повезло, попробуйте еще раз\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a9b04a5",
      "metadata": {
        "id": "3a9b04a5"
      },
      "source": [
        "Давайте сделаем задание немного прикольнее и изменим одну из колонок. Как мы знаем, полная мультиколлинеарность запрещает нам пользоваться аналитическим решением, но `sklearn` по какой-то причине это обходит, хмм"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d905e03b",
      "metadata": {
        "id": "d905e03b"
      },
      "outputs": [],
      "source": [
        "x[:, 3] = x[:, 2] + x[:, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ee87220",
      "metadata": {
        "id": "2ee87220"
      },
      "outputs": [],
      "source": [
        "sklearn_linreg = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
        "sklearn_linreg.fit(x, y)\n",
        "print(\"Sklearn MSE\", mse(sklearn_linreg.predict(x), y))\n",
        "\n",
        "your_linreg = CustomLinearRegression(AnalyticSolutionOptimizer(), loss_function=MSELoss())\n",
        "your_linreg.fit(x, y)\n",
        "print(\"Your MSE\", mse(your_linreg.predict(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8315596",
      "metadata": {
        "id": "a8315596"
      },
      "source": [
        "Ваша задача - понять, как можно сделать так, чтобы аналитическое решение работало всегда, вне зависимости от матрицы X. Как оказывается, это можно сделать, если воспользоваться SVD разложением. Для имплементации воспользуйтесь `scipy.sparse.linalg.svds`.\n",
        "\n",
        "- Выведите через SVD формулу оптимальных $w$ в задаче минимизации MSE.\n",
        "\n",
        "- Имплементируйте подсчет этого решения в `MSELoss._svd_analytic_solution`\n",
        "    - Мир итерационных агоритмов причудлив. Если вы посмотрите на опции солверов svds, то увидите, что возможности вычислить точно все сингулярные числа вам не дают (propack рандомизированный и даст вам неточные ответы). Используйте стандартный солвер, выставьте максимальную доступную точность.\n",
        "\n",
        "- Ответьте на **вопрос на засыпку**. Вообще говоря, в ряде случаев (например в нашем), даже такая неабсолютная на первый взгляд точность все равно позволяет получить точное решение задачи. Обоснуйте, почему? Как называется такой вид SVD? Какого минимального числа сингулярных чисел с вероятностью 1 будет достаточно в нашем случае для получения точного решения? Обоснуйте, почему.\n",
        "\n",
        "\n",
        "$$\\text{X} = \\underset{n\\times m}{\\mathrm{U}} \\ \\underset{m\\times m}{\\mathrm{\\Sigma}} \\ \\underset{m\\times k}{\\mathrm{V^T}}$$\n",
        "$$ w = $$\n",
        "\n",
        "**Ответ**: `# your text here (ಠ.ಠ)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "661ae116",
      "metadata": {
        "id": "661ae116"
      },
      "outputs": [],
      "source": [
        "sklearn_linreg = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
        "sklearn_linreg.fit(x, y)\n",
        "print(\"Sklearn MSE\", mse(sklearn_linreg.predict(x), y))\n",
        "\n",
        "\n",
        "your_linreg =  CustomLinearRegression(AnalyticSolutionOptimizer(),\n",
        "                                      loss_function=MSELoss(analytic_solution_func=MSELoss._svd_analytic_solution))\n",
        "your_linreg.fit(x, y)\n",
        "\n",
        "print(\"Your MSE\", mse(your_linreg.predict(x), y))\n",
        "\n",
        "assert abs(mse(your_linreg.predict(x), y) - mse(sklearn_linreg.predict(x), y)) < 1e-12, \"Не повезло, попробуйте еще раз\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09006ad9",
      "metadata": {
        "id": "09006ad9"
      },
      "source": [
        "## Задание 2. Реализация градиентного спуска (4 балла)\n",
        "\n",
        "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле `descents.py`. При помощи них мы будем искать итеративные решения, подавая оптимизаторы внутрь нашей `CustomLinearRegression`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dbb06bd",
      "metadata": {
        "id": "5dbb06bd"
      },
      "source": [
        "### Напоминание про градиентный спуск\n",
        "\n",
        "Основное свойство антиградиента &ndash; он указывает в сторону *наискорейшего* убывания функции в данной точке. Соответственно, будет логично стартовать из некоторой точки, сдвинуться в сторону антиградиента,\n",
        "пересчитать антиградиент и снова сдвинуться в его сторону и т.д. Запишем это более формально.\n",
        "\n",
        "Пусть $w_0$ &ndash; начальный набор параметров (например, нулевой или сгенерированный из некоторого\n",
        "случайного распределения). Тогда ванильный градиентный спуск состоит в повторении следующих шагов до сходимости:\n",
        "\n",
        "$$\n",
        "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
        "$$\n",
        "\n",
        "Здесь $\\eta_{k}$ обозначает длину шага на $k$-ой итерации (learning rate), а $Q(w)$ - функцию потерь (loss function).\n",
        "\n",
        "Градиент для MSE вы уже нашли выше"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e80d2b",
      "metadata": {
        "id": "89e80d2b"
      },
      "source": [
        "### Задание 2.0. Learning Rate Schedules (0.2 балла)\n",
        "\n",
        "Обратите внимание на **абстрактный** класс `LearningRateSchedule` в файле `descents.py`. С помощью его имплементаций мы на каждой итерации градиентного спуска будем получать соответствующий `learning_rate` $\\eta_k$.\n",
        "\n",
        "В файле уже реализован класс `ConstantLR`, который на каждой итерации возвращает один и тот же заранее заданный шаг. **Ваша задача в этом пункте – реализовать `TimeDecayLR`**, который мы будем использовать для обучения линейной регрессии. Формула очередного шага должна выглядеть следующим образом:\n",
        "$$\n",
        "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
        "$$\n",
        "\n",
        "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc313e5e",
      "metadata": {
        "id": "bc313e5e"
      },
      "source": [
        "### Задание 2.1. Родительский класс BaseDescent (1 балл).\n",
        "\n",
        "\n",
        "Внимательно изучите устройство класса `BaseDescent`. У него есть один непомеченный абстрактным метод, который ему как частичному наследнику абстрактного класса нужно имплементировать - это `optimize`. В\n",
        "этом методе необходимо имплементировать основной цикл обучения, и далее его будут переиспользовать все его наследники.\n",
        "\n",
        "- **Допишите метод `BaseDescent.optimize`**\n",
        "\n",
        "\n",
        "Для этого и всех дальнейших заданий необходимо соблюдать следующие условия:\n",
        "\n",
        "* **Все вычисления должны быть векторизованы;**\n",
        "* Циклы средствами python допускаются только для итераций градиентного спуска;\n",
        "* В качестве критерия останова необходимо использовать (одновременно):\n",
        "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`;\n",
        "    * Разность весов содержит наны;\n",
        "    * Достижение максимального числа итераций `max_iter`.\n",
        "* Будем считать, что все данные, которые поступают на вход имеют столбец единичек последним столбцом;\n",
        "* Веса модели надо обновлять внутри функции `_update_weights`, она неспроста так называется\n",
        "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `CustomLinearRegression.loss_history`, в нём будем хранить *значения функции потерь до каждого шага, начиная с нулевого* (до первого шага по антиградиенту) и *значение функции потерь после оптимизации*.\n",
        "\n",
        "\n",
        "Обратите внимание, что метод `_update_weights` всё ещё является абстрактным - его все ещё должны будут имплементировать дальнейшие наследники; фактически, только способом обновления весов они и отличаются. Она должна должна обновлять веса модели `self.model.w`, а также возвращать величину обновления $w_{k + 1} - w_k$.\n",
        "\n",
        "Также обратите внимание на атрибут `self.iteration`, отвечающий за номер итерации алгоритма спуска. Как раз с помощью него (и `self.lr_schedule`) мы и будем получать `learning_rate` на соответствующей итерации алгоритма."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VU9cU-ax_uFR",
      "metadata": {
        "id": "VU9cU-ax_uFR"
      },
      "source": [
        "**Обратите внимание**\n",
        "\n",
        "*да, в третий раз*\n",
        "\n",
        "Все реализуемые вами классы спуска в задании - это *универсальные* оптимизаторы. Они не должны считать градиенты конкретной функции потерь внутри себя.\n",
        "\n",
        "Для вычисления градиента они всегда обращаются к модели, с которой работают:\n",
        "\n",
        "```\n",
        "gradient = self.model.compute_gradients(X_batch, y_batch)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9b83e1",
      "metadata": {
        "id": "3f9b83e1"
      },
      "source": [
        "### Задание 2.2. Полный градиентный спуск VanillaGradientDescent (0.6 балла).\n",
        "\n",
        "Реализуйте полный градиентный спуск заполнив пропуски в классе `VanillaGradientDescent` в файле `descents.py`. Напомним, что шаг классического градиентного спуска выглядит следующим образом:\n",
        "\n",
        "$$\n",
        "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
        "$$\n",
        "\n",
        "**Важно**: Здесь и далее функция `_update_weights` должна возвращать разницу между $w_{k + 1}$ и $w_{k}$: $\\quad w_{k + 1} - w_{k} = -\\eta_{k} \\nabla_{w} Q(w_{k})$. Кроме того, соответственно своему названию, она должна обновлять веса модели `model.w`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f63d3f4",
      "metadata": {
        "id": "1f63d3f4"
      },
      "source": [
        "### Напоминание про SGD (стохастических градиентный спуск)\n",
        "\n",
        "Как правило, в задачах машинного обучения функционал $Q(w)$ представим в виде суммы $\\ell$ функций:\n",
        "\n",
        "$$\n",
        "    Q(w)\n",
        "    =\n",
        "    \\frac{1}{\\ell}\n",
        "    \\sum_{i = 1}^{\\ell}\n",
        "        q_i(w).\n",
        "$$\n",
        "\n",
        "В нашем домашнем задании отдельные функции $q_i(w)$ соответствуют ошибкам на отдельных объектах.\n",
        "\n",
        "Проблема метода градиентного спуска состоит в том, что на каждом шаге необходимо вычислять градиент всей суммы (будем его называть полным градиентом):\n",
        "\n",
        "$$\n",
        "    \\nabla_w Q(w)\n",
        "    =\n",
        "    \\frac{1}{\\ell}\n",
        "    \\sum_{i = 1}^{\\ell}\n",
        "        \\nabla_w q_i(w).\n",
        "$$\n",
        "\n",
        "Это может быть очень трудоёмко при больших размерах выборки. В то же время точное вычисление градиента может быть не так уж необходимо &ndash; как правило, мы делаем не очень большие шаги в сторону антиградиента, и наличие в нём неточностей не должно сильно сказаться на общей траектории.\n",
        "\n",
        "Оценить градиент суммы функций можно средним градиентов случайно взятого подмножества функций:\n",
        "\n",
        "$$\n",
        "    \\nabla_{w} Q(w_{k}) \\approx \\dfrac{1}{|B|}\\sum\\limits_{i \\in B}\\nabla_{w} q_{i}(w_{k}),\n",
        "$$\n",
        "где $B$ - это случайно выбранное подмножество индексов, обычно называемое **батчом**.\n",
        "\n",
        "Оценка $\\frac{1}{|B|} \\sum \\limits_{i \\in B} \\nabla_w q_i(w_k)$ называется **стохастическим градиентом** функции потерь, а получившийся метод называют методом **стохастического градиентного спуска** или просто SGD."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7528caa0",
      "metadata": {
        "id": "7528caa0"
      },
      "source": [
        "### Задание 2.3. Стохастический градиентный спуск StochasticGradientDescent (0.7 балла).\n",
        "\n",
        "Реализуйте стохастический градиентный спуск, заполнив пропуски в классе `StochasticGradientDescent`. Для оценки градиента используйте формулу выше (среднее градиентов случайно выбранного батча объектов). Шаг оптимизации:\n",
        "\n",
        "$$\n",
        "    w_{k + 1} = w_{k} - \\eta_{k} \\dfrac{1}{|B|}\\sum\\limits_{i \\in B}\\nabla_{w} q_{i}(w_{k}).\n",
        "$$\n",
        "\n",
        "Размер батча будет являться **гиперпараметром** метода и передаваться в конструктор класса `__init__(...)`. Семплировать индексы батча объектов $B$ можно с повторениями (через np.random.randint) - это допустимо и даёт несмещённую оценку градиента. По желанию можно без повторений (np.random.choice(..., replace=False) или через пермутацию по эпохам)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o965aNOn2Owe",
      "metadata": {
        "id": "o965aNOn2Owe"
      },
      "source": [
        "### Задание 2.4 Stochastic Average Gradient (0.6 балла)\n",
        "\n",
        "Держим память последних индивидуальных градиентов $g_i$ по всем объектам и их среднее $\\bar g = \\frac{1}{\\ell}\\sum_i g_i$. На каждом шаге выбираем индексы $j$ (мини-батч), заново считаем $g_j^{new}(w_k)$, обновляем среднее:\n",
        "$$\n",
        "\\bar g \\leftarrow \\bar g + \\frac{1}{\\ell}\\bigl(g_j^{new} - g_j^{old}\\bigr),\\qquad\n",
        "w_{k+1} = w_k - \\eta_k \\bar g.\n",
        "$$\n",
        "Инициализация: $g_i=0 \\Rightarrow \\bar g=0$.\n",
        "\n",
        "Так получаем шаг почти как у полного градиента, но считаем градиент лишь на нескольких объектах за итерацию.\n",
        "\n",
        "Реализуйте класс `SAGDescent` в `descents.py` с хранением `grad_memory` и `avg_grad`. Подсказка: чтобы получить пер-объектный градиент, можно вызывать `compute_gradients` на срезе из одного объекта `X[j:j+1]` или на фильтрованной индексами матрице для батча.\n",
        "\n",
        "**Имейте в виду, что SAG достаточно капризный**: для его сходимости (и ее скорости) достаточно важен размер батча. Для вас установлено дефолтное значение, но на реальных данных его может быть недостаточно. В сравнениях методов ниже вам может понадобится подобрать значение размера батча, чтобы раскрыть потенциал метода. То ж касается и SGD, но в меньшей степени."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c9b391",
      "metadata": {
        "id": "d7c9b391"
      },
      "source": [
        "### Напоминание про метод инерции (или метод моментов)\n",
        "\n",
        "Может оказаться, что направление антиградиента сильно меняется от шага к шагу. Например, если линии уровня функционала сильно вытянуты, то из-за ортогональности градиента линиям уровня он будет менять направление на почти противоположное на каждом шаге. Такие осцилляции будут вносить сильный шум в движение, и процесс оптимизации займёт много итераций. Чтобы избежать этого, можно усреднять векторы антиградиента с нескольких предыдущих шагов &ndash; в этом случае шум уменьшится, и такой средний вектор будет указывать в сторону общего направления движения. Введём для этого вектор инерции:\n",
        "\n",
        "\\begin{align}\n",
        "    &h_0 = 0, \\\\\n",
        "    &h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_w Q(w_{k})\n",
        "\\end{align}\n",
        "\n",
        "Здесь $\\alpha$ &ndash; параметр метода, определяющей скорость затухания градиентов с предыдущих шагов. Разумеется, вместо вектора градиента может быть использована его аппроксимация (например, в случае **стохастического градиентного спуска**). Чтобы сделать шаг градиентного спуска, просто сдвинем предыдущую точку на вектор инерции:\n",
        "\n",
        "$$\n",
        "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
        "$$\n",
        "\n",
        "Заметим, что если по какой-то координате градиент постоянно меняет знак, то в результате усреднения градиентов в векторе инерции эта координата окажется близкой к нулю. Если же по координате знак градиента всегда одинаковый, то величина соответствующей координаты в векторе инерции будет большой, и мы будем делать большие шаги в соответствующем направлении."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f53187c3",
      "metadata": {
        "id": "f53187c3"
      },
      "source": [
        "### Задание 2.5 Метод Momentum - MomentumDescent (0.5 балла).\n",
        "\n",
        "Реализуйте градиентный спуск с методом инерции заполнив пропуски в классе `MomentumDescent`. Шаг оптимизации:\n",
        "\n",
        "\\begin{align}\n",
        "    &h_0 = 0, \\\\\n",
        "    &h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_w Q(w_{k}) \\\\\n",
        "    &w_{k + 1} = w_{k} - h_{k + 1}.\n",
        "\\end{align}\n",
        "\n",
        "$\\alpha$ являеться гиперпараметром метода, однако в данном домашнем задании мы зафиксируем её за вас $\\alpha = 0.9$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "602977f8",
      "metadata": {
        "id": "602977f8"
      },
      "source": [
        "### Напоминание про AdaGrad, RMSprop и Adam\n",
        "\n",
        "Градиентный спуск очень чувствителен к выбору длины шага. Если шаг большой, то есть риск, что мы будем перескакивать через точку минимума; если же шаг маленький, то для нахождения минимума потребуется много итераций. При этом нет способов заранее определить правильный размер шага &ndash; к тому же, схемы с постепенным уменьшением шага по мере итераций могут тоже плохо работать.\n",
        "\n",
        "В методе AdaGrad предлагается сделать свою длину шага для каждой компоненты вектора параметров. Идея проста: мы будем \"копить\" сумму квадратов градиентов и делить очередной градиент на корень из этой суммы. Таким образом, обновление весов с большими градиентами будет тормозиться, а с маленькими наоборот получать большие шаги. Формула обновлени будет выглядить так:\n",
        "\n",
        "\\begin{align}\n",
        "    &G_{kj} = G_{k-1,j} + (\\nabla_w Q(w_{k - 1}))_j^2; \\\\\n",
        "    &w_{jk} = w_{j,k-1} - \\frac{\\eta_t}{\\sqrt{G_{kj}} + \\varepsilon} (\\nabla_w Q(w_{k - 1}))_j.\n",
        "\\end{align}\n",
        "\n",
        "Здесь $\\varepsilon$ небольшая константа, которая предотвращает деление на ноль.\n",
        "\n",
        "В данном методе можно зафиксировать длину шага (например, $\\eta_k = 0.01$) и не подбирать её в процессе обучения **(обратите внимание, что в данном домашнем задании длина шага не фиксируется)**. Отметим, что данный метод подходит для разреженных задач, в которых у каждого объекта большинство признаков равны нулю. Для признаков, у которых ненулевые значения встречаются редко, будут делаться большие шаги; если же какой-то признак часто является ненулевым, то шаги по нему будут небольшими.\n",
        "\n",
        "У метода AdaGrad есть большой недостаток: переменная $G_{kj}$ монотонно растёт, из-за чего шаги становятся всё медленнее и могут остановиться ещё до того, как достигнут минимум функционала. Проблема решается в методе RMSprop, где используется экспоненциальное затухание градиентов:\n",
        "\n",
        "$$\n",
        "    G_{kj} = \\alpha G_{k-1,j} + (1 - \\alpha) (\\nabla_w Q(w^{(k-1)}))_j^2.\n",
        "$$\n",
        "\n",
        "В этом случае размер шага по координате зависит в основном от того, насколько\n",
        "быстро мы двигались по ней на последних итерациях.\n",
        "\n",
        "Можно объединить идеи описанных выше методов: накапливать градиенты со всех прошлых шагов для\n",
        "избежания осцилляций (метод инерции), а также делать адаптивную длину шага по каждому параметру (`RMSProp`). Таким образом, мы получим метод `Adam` с той лишь разницей, что в методе `Adam` дополнительно делается нормировка накопленных градиентов и квадратов градиентов для устранения смещения."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1010de93",
      "metadata": {
        "id": "1010de93"
      },
      "source": [
        "### Задание 2.6. Метод Adam (Adaptive Moment Estimation) (0.4 балла).\n",
        "\n",
        "Реализуйте градиентный спуск с методом Adam, заполнив пропуски в классе `Adam`. Шаг оптимизации:\n",
        "\n",
        "\\begin{align}\n",
        "    &m_0 = 0, \\quad v_0 = 0; \\\\ \\\\\n",
        "    &m_{k + 1} = \\beta_1 m_k + (1 - \\beta_1) \\nabla_w Q(w_{k}); \\\\ \\\\\n",
        "    &v_{k + 1} = \\beta_2 v_k + (1 - \\beta_2) \\left(\\nabla_w Q(w_{k})\\right)^2; \\\\ \\\\\n",
        "    &\\widehat{m}_{k} = \\dfrac{m_k}{1 - \\beta_1^{k}}, \\quad \\widehat{v}_{k} = \\dfrac{v_k}{1 - \\beta_2^{k}}; \\\\ \\\\\n",
        "    &w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\widehat{v}_{k + 1}} + \\varepsilon} \\widehat{m}_{k + 1}.\n",
        "\\end{align}\n",
        "\n",
        "$\\beta_1 = 0.9, \\beta_2 = 0.999$ и $\\varepsilon = 10^{-8}$ будут зафиксированы за вас."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZNtnKIk9LVsT",
      "metadata": {
        "id": "ZNtnKIk9LVsT"
      },
      "source": [
        "## Задание 3. Проверка кода (0 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WD28hZzlLNAB",
      "metadata": {
        "id": "WD28hZzlLNAB"
      },
      "source": [
        "Данная секция нужна для того, чтобы убедиться в правильности реализации методов спуска и класса `CustomLinearRegression`. В начале мы сделаем небольшую локальную проверку на \"адекватность\" и \"запускаемость\" ваших моделей, после чего уже можно будет делать посылки в Яндекс Контест."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "USsgi0BdLElg",
      "metadata": {
        "id": "USsgi0BdLElg"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XyD-g7nPLBnw",
      "metadata": {
        "id": "XyD-g7nPLBnw"
      },
      "outputs": [],
      "source": [
        "#%autoreload 2\n",
        "\n",
        "from descents import (\n",
        "    VanillaGradientDescent,\n",
        "    StochasticGradientDescent,\n",
        "    SAGDescent,\n",
        "    MomentumDescent,\n",
        "    Adam\n",
        ")\n",
        "\n",
        "from linear_regression import CustomLinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b249061",
      "metadata": {
        "id": "5b249061"
      },
      "outputs": [],
      "source": [
        "num_objects = 100\n",
        "dimension = 5\n",
        "\n",
        "x = np.random.rand(num_objects, dimension)\n",
        "y = np.random.rand(num_objects)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ONltS_wWK8ID",
      "metadata": {
        "id": "ONltS_wWK8ID"
      },
      "source": [
        "Проверяем код на запускаемость."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DplvivRvK5JN",
      "metadata": {
        "id": "DplvivRvK5JN"
      },
      "outputs": [],
      "source": [
        "descent_models = [\n",
        "   VanillaGradientDescent,\n",
        "   StochasticGradientDescent,\n",
        "   SAGDescent,\n",
        "   MomentumDescent,\n",
        "   Adam\n",
        "]\n",
        "\n",
        "max_iter = 10\n",
        "tolerance = 0\n",
        "num_objects = 100\n",
        "dimension = 5\n",
        "\n",
        "for descent_model in descent_models:\n",
        "   optimizer = descent_model(tolerance=tolerance, max_iter=max_iter)\n",
        "   model = CustomLinearRegression(optimizer=optimizer)\n",
        "   model.fit(x, y)\n",
        "   assert len(model.loss_history) == max_iter + 1, \"Loss history failed\"\n",
        "   y_pred = model.predict(x)\n",
        "   assert y_pred.shape == y.shape, \"Prediction shape does not match target variable\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LhBs8BjxK1KL",
      "metadata": {
        "id": "LhBs8BjxK1KL"
      },
      "source": [
        "Если ваше решение прошло все тесты локально, то теперь пришло время протестировать его в Яндекс Контесте [To Be Announced].\n",
        "\n",
        "Для каждой задачи из контеста вставьте ID успешной посылки и ваш ник (почту):\n",
        "\n",
        "* **Ник/почта**:\n",
        "\n",
        "\n",
        "* **VanillaGradientDescent**:\n",
        "\n",
        "\n",
        "* **StochasticGradientDescent**:\n",
        "\n",
        "\n",
        "* **SAGDescent**:\n",
        "\n",
        "\n",
        "* **MomentumDescent**:\n",
        "\n",
        "\n",
        "* **Adam**:\n",
        "\n",
        "\n",
        "* **LinearRegression**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5586dcbc",
      "metadata": {
        "id": "5586dcbc"
      },
      "source": [
        "## Задание 4. Работа с данными (1 балл)\n",
        "\n",
        "Мы будем использовать датасет объявлений по продаже машин на немецком Ebay. В задаче предсказания целевой переменной для нас будет являться цена."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67a0d7c6",
      "metadata": {
        "id": "67a0d7c6"
      },
      "source": [
        "* Постройте график распределения целевой переменной в данных, подумайте, нужно ли заменить её на логарифм. Присутствуют ли выбросы в данных с аномальной ценой? Если да, то удалите их из данных.\n",
        "\n",
        "* Проведите исследование данных:\n",
        "    * Проанализируйте тип столбцов, постройте графики зависимости целевой переменной от признака, распределения значений признака;\n",
        "    * Подумайте (и напишите): какие признаки могут быть полезными на основе этих графиков, обработайте выбросы;\n",
        "    * Подумайте (и напишите): какие трансформации признаков из известных вам будет уместно применить;\n",
        "    * Разделите полезные признаки на категориальные, вещественные и те, которые не надо предобрабатывать.\n",
        "* Разделите данные на обучающую, валидационную и тестовую выборки в отношении 8:1:1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28bb408",
      "metadata": {
        "id": "b28bb408"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd  # при желании, можете заменить на polars/pyspark или что угодно, что вам нравится\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from descents import (\n",
        "    ConstantLR, TimeDecayLR,\n",
        "    VanillaGradientDescent, StochasticGradientDescent,\n",
        "    MomentumDescent, Adam, SAGDescent\n",
        ")\n",
        "from linear_regression import CustomLinearRegression\n",
        "\n",
        "sns.set(style='darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be065ca",
      "metadata": {
        "id": "2be065ca"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('autos.csv')  # разумеется, если вы используете не pandas, это надо поменять"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6055f2b2",
      "metadata": {
        "id": "6055f2b2",
        "outputId": "2ccdd548-beaa-4068-fdca-49ed12016c16"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>brand</th>\n",
              "      <th>model</th>\n",
              "      <th>vehicleType</th>\n",
              "      <th>gearbox</th>\n",
              "      <th>fuelType</th>\n",
              "      <th>notRepairedDamage</th>\n",
              "      <th>powerPS</th>\n",
              "      <th>kilometer</th>\n",
              "      <th>autoAgeMonths</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>volkswagen</td>\n",
              "      <td>golf</td>\n",
              "      <td>kleinwagen</td>\n",
              "      <td>manuell</td>\n",
              "      <td>benzin</td>\n",
              "      <td>nein</td>\n",
              "      <td>75</td>\n",
              "      <td>150000</td>\n",
              "      <td>177</td>\n",
              "      <td>1500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>skoda</td>\n",
              "      <td>fabia</td>\n",
              "      <td>kleinwagen</td>\n",
              "      <td>manuell</td>\n",
              "      <td>diesel</td>\n",
              "      <td>nein</td>\n",
              "      <td>69</td>\n",
              "      <td>90000</td>\n",
              "      <td>93</td>\n",
              "      <td>3600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bmw</td>\n",
              "      <td>3er</td>\n",
              "      <td>limousine</td>\n",
              "      <td>manuell</td>\n",
              "      <td>benzin</td>\n",
              "      <td>ja</td>\n",
              "      <td>102</td>\n",
              "      <td>150000</td>\n",
              "      <td>246</td>\n",
              "      <td>650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>peugeot</td>\n",
              "      <td>2_reihe</td>\n",
              "      <td>cabrio</td>\n",
              "      <td>manuell</td>\n",
              "      <td>benzin</td>\n",
              "      <td>nein</td>\n",
              "      <td>109</td>\n",
              "      <td>150000</td>\n",
              "      <td>140</td>\n",
              "      <td>2200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mazda</td>\n",
              "      <td>3_reihe</td>\n",
              "      <td>limousine</td>\n",
              "      <td>manuell</td>\n",
              "      <td>benzin</td>\n",
              "      <td>nein</td>\n",
              "      <td>105</td>\n",
              "      <td>150000</td>\n",
              "      <td>136</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        brand    model vehicleType  gearbox fuelType notRepairedDamage  \\\n",
              "0  volkswagen     golf  kleinwagen  manuell   benzin              nein   \n",
              "1       skoda    fabia  kleinwagen  manuell   diesel              nein   \n",
              "2         bmw      3er   limousine  manuell   benzin                ja   \n",
              "3     peugeot  2_reihe      cabrio  manuell   benzin              nein   \n",
              "4       mazda  3_reihe   limousine  manuell   benzin              nein   \n",
              "\n",
              "   powerPS  kilometer  autoAgeMonths  price  \n",
              "0       75     150000            177   1500  \n",
              "1       69      90000             93   3600  \n",
              "2      102     150000            246    650  \n",
              "3      109     150000            140   2200  \n",
              "4      105     150000            136   2000  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2be799d",
      "metadata": {
        "id": "c2be799d"
      },
      "source": [
        "Колонки в данных:\n",
        "\n",
        "* `brand` - название бренда автомобиля\n",
        "* `model` - название модели автомобиля\n",
        "* `vehicleType` - тип транспортного средства\n",
        "* `gearbox` - тип трансмисcии\n",
        "* `fuelType` - какой вид топлива использует автомобиль\n",
        "* `notRepairedDamage` - есть ли в автомобиле неисправность, которая еще не устранена\n",
        "* `powerPS` - мощность автомобиля в PS (метрическая лошадиная сила)\n",
        "* `kilometer` - сколько километров проехал автомобиль, пробег\n",
        "* `autoAgeMonths` - возраст автомобиля в месяцах\n",
        "\n",
        "\n",
        "* `price` - цена, указанная в объявлении о продаже автомобиля (целевая переменная)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569842b1",
      "metadata": {
        "id": "569842b1"
      },
      "source": [
        "Разделите признаки на категориальные, числовые и ... все остальное"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184bc325",
      "metadata": {
        "id": "184bc325"
      },
      "outputs": [],
      "source": [
        "categorical = []\n",
        "numeric = []\n",
        "other = []\n",
        "\n",
        "# YOUR CODE (EDA) HERE ٩(⁎❛ᴗ❛⁎)۶"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c751ce80",
      "metadata": {
        "id": "c751ce80"
      },
      "source": [
        "Добавляем в данные единичную колонку `bias`, чтобы не делать отдельные параметр $b$ для свободного члена модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8351dbf",
      "metadata": {
        "id": "a8351dbf"
      },
      "outputs": [],
      "source": [
        "data['bias'] = 1\n",
        "other += ['bias']\n",
        "\n",
        "x = data[categorical + numeric + other]\n",
        "y = data['price']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь вам необходимо разбить данные на обучающую, тестовую и валидационную выборки."
      ],
      "metadata": {
        "id": "JVbZiIpfy7fl"
      },
      "id": "JVbZiIpfy7fl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af594fc",
      "metadata": {
        "id": "3af594fc"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ┌(ಠ_ಠ)┘"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14b5798",
      "metadata": {
        "id": "c14b5798"
      },
      "source": [
        "А также сделаем базовую обработку данных, а именно:\n",
        "* Применим `OneHotEncoding` к категориальным признакам\n",
        "* Стандартизуем численные признаки с помощью `StandardScaler`\n",
        "* Остальные признаки трогать не будем, т.к. с ними непонятно что делать"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> А почему мы сначала делим данные, а только потом применяем обработку данных? Энкодеры и скейлеры используют информацию о данных: если сделать fit на всем датасете до split, это будет утечка: статистики из val/test попадут в обучение."
      ],
      "metadata": {
        "id": "sinEmsWDzj5I"
      },
      "id": "sinEmsWDzj5I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f618ad",
      "metadata": {
        "id": "c4f618ad"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
        "    ('scaling', StandardScaler(), numeric),\n",
        "    ('other',  'passthrough', other)\n",
        "])\n",
        "\n",
        "X_train = column_transformer.fit_transform(X_train)\n",
        "X_val = column_transformer.transform(X_val)\n",
        "X_test = column_transformer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "901a343f",
      "metadata": {
        "id": "901a343f"
      },
      "source": [
        "## Задание 5. Сравнение методов градиентного спуска (1.5 балла)\n",
        "\n",
        "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e877ce0",
      "metadata": {
        "id": "2e877ce0"
      },
      "source": [
        "### Задание 5.1. Подбор оптимальной длины шага (0.75 балла)\n",
        "\n",
        "Подберите по валидационной выборке наилучшую длину шага $\\lambda$ для каждого метода с точки зрения ошибки. Для этого сделайте перебор по логарифмической сетке. Для каждого метода посчитайте ошибку на обучающей и тестовой выборках, посчитайте качество по метрике $R^2$, сохраните количество итераций до сходимости.\n",
        "\n",
        "Все параметры кроме `lambda_` стоит выставить равным значениям по умолчанию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b85c124b",
      "metadata": {
        "id": "b85c124b"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE (⌐■_■)\n",
        "\n",
        "# to find optimal lambda_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07913925",
      "metadata": {
        "id": "07913925"
      },
      "source": [
        "### Задание 5.2. Сравнение методов (0.75 балла)\n",
        "\n",
        "Постройте график зависимости ошибки на обучающей выборке от номера итерации (все методы на одном графике).\n",
        "\n",
        "Посмотрите на получившиеся результаты (таблички с метриками и график). Сравните методы между собой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c61f205",
      "metadata": {
        "id": "4c61f205"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE (づ｡◕‿‿◕｡)づ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46d325d5",
      "metadata": {
        "id": "46d325d5"
      },
      "source": [
        "## Задание 6. Стохастический градиентный спуск и размер батча (1 балл)\n",
        "\n",
        "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска.\n",
        "\n",
        "* Сделайте по несколько запусков (например, $k = 10$) стохастического градиентного спуска на обучающей выборке для каждого размера батча из перебираемого списка. Замерьте время в секундах и количество итераций до сходимости. Посчитайте среднее этих значений для каждого размера батча.\n",
        "* Постройте график зависимости количества шагов до сходимости от размера батча. _(под сходимостью понимается достижение критерия останова)_\n",
        "* Постройте график зависимости времени до сходимости от размера батча.\n",
        "\n",
        "Посмотрите на получившиеся результаты. Какие выводы можно сделать про подбор размера батча для стохастического градиентного спуска?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c5b5a72",
      "metadata": {
        "id": "0c5b5a72"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE (ง •̀_•́)ง"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daed9a42",
      "metadata": {
        "id": "daed9a42"
      },
      "source": [
        "**Выводы:**\n",
        "\n",
        "`# your text here (ಠ.ಠ)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cafe313c",
      "metadata": {
        "id": "cafe313c"
      },
      "source": [
        "## Задание 7. Регуляризация (0.5 балла)\n",
        "\n",
        "В этом задании вам предстоит исследовать влияние регуляризации на работу различных методов градиентного спуска. Напомним, регуляризация – это добавка к функции потерь, которая штрафует за норму весов. Мы будем использовать $L_2$-регуляризацию, таким образом функция потерь приобретает следующий вид:\n",
        "\n",
        "$$\n",
        "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2 + \\dfrac{\\mu}{2} \\| w \\|^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f33ceac8",
      "metadata": {
        "id": "f33ceac8"
      },
      "source": [
        "Допишите класс `linear_regression.L2Regularization`, следуя интерфейсу и докуметации.\n",
        "\n",
        "Используя регуляризованный лосс в эксприментах, найдите лучшие параметры обучения с регуляризацией аналогично 5 заданию. Будем подбирать длину шага $\\lambda$ (`lambda_`) и коэффициент регуляризации $\\mu$ (`mu`).\n",
        "\n",
        "Сравните для каждого метода результаты с регуляризацией и без регуляризации (нужно опять сохранить ошибку и качество по метрике $R^2$ на обучающей и тестовой выборках и количество итераций до сходимости).\n",
        "\n",
        "Постройте для каждого метода график со значениями функции потерь MSE с регуляризацией и без регуляризации (всего должно получиться 5 графиков).\n",
        "\n",
        "Посмотрите на получившиеся результаты. Какие можно сделать выводы, как регуляризация влияет на сходимость? Как изменилось качество на обучающей выборке? На тестовой? Чем вы можете объяснить это?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36793e98",
      "metadata": {
        "id": "36793e98"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE (✿◠‿◠)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3272a36d",
      "metadata": {
        "id": "3272a36d"
      },
      "source": [
        "**Вывод:**\n",
        "\n",
        "`# your text here (ಠ.ಠ)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae940767",
      "metadata": {
        "id": "ae940767"
      },
      "source": [
        "## Задание 8. Альтернативные функции потерь (1 балл)\n",
        "\n",
        "В этом задании вам предстоит использовать другую функцию потерь для нашей задачи регрессии. В качестве функции потерь мы выбрали **LogCosh** и **HuberLoss**:\n",
        "\n",
        "$$\n",
        "    L(y, a)\n",
        "    =\n",
        "    \\log\\left(\\cosh(a - y)\\right).\n",
        "$$\n",
        "\n",
        "$$\n",
        "L_{\\text{Huber}}(y, a) = \\frac{1}{n} \\sum_{i=1}^{n}\n",
        "\\begin{cases}\n",
        "   \\frac{1}{2} (a_i - y_i)^2, & \\text{если } |a_i - y_i| < \\delta, \\\\\n",
        "   \\delta \\cdot |a_i - y_i| - \\frac{1}{2} \\delta^2, & \\text{если } |a_i - y_i| \\geq \\delta,\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0a740a2",
      "metadata": {
        "id": "f0a740a2"
      },
      "source": [
        "Самостоятельно продифференцируйте данные функции потерь чтобы найти их градиенты _(требуется показать не только результат, но и промежуточные вычисления)_:\n",
        "\n",
        "**Решение:**\n",
        "\n",
        "`# your solution here (ಠ.ಠ)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d9566b6",
      "metadata": {
        "id": "4d9566b6"
      },
      "source": [
        "Программно реализуйте функции потерь и их градиенты для LogCosh и HuberLoss в файле `linear_regression.py`. После этого обучите все пять методов градиентного спуска (без регуляризации) с этими лоссами аналогично заданию 5 и сравните качество с результатами из задания 5, где использовался MSE.\n",
        "\n",
        "Имплементировать эти функции потерь необходимо при помощи наследования от `linear_regression.LossFunction` и имплементирования всех абстрактных методов. Аналитическое решение для этих функций выводить и имплементировать не требуется.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v14GEDtWHKM7",
      "metadata": {
        "id": "v14GEDtWHKM7"
      },
      "source": [
        "### Кулинарно-социализационный бонус. (0.5 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "al1GZr40pSEm",
      "metadata": {
        "id": "al1GZr40pSEm"
      },
      "source": [
        "Как мы знаем, в феврале зима уже не объяснение, а обстоятельство непреодолимой силы - и лучший способ спорить с ней это поделиться теплом с друзьями и близкими. Выберите рецепт, который соответствует вашему настроению, приготовьте выбранное блюдо и угостите хотя бы одного человека им. Кратко опишите ваши впечатления, прикрепите рецепт и фотографии блюда и довольного гостя."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hse_ml_venv_kernel",
      "language": "python",
      "name": "hse_ml_venv_kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
